{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:33: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3996 images belonging to 20 classes.\n",
      "Found 1250 images belonging to 20 classes.\n",
      "Evaluating the model...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected all entries in the `metrics` list to be metric objects. Received instead:\nmetrics=[[<MeanMetricWrapper name=accuracy>]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 47\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the validation dataset\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating the model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 47\u001b[0m val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Validation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Make predictions on the validation set\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\compile_utils.py:233\u001b[0m, in \u001b[0;36mCompileMetrics._build_metrics_set\u001b[1;34m(self, metrics, num_outputs, output_names, y_true, y_pred, argument_name)\u001b[0m\n\u001b[0;32m    231\u001b[0m             metrics \u001b[38;5;241m=\u001b[39m [metrics]\n\u001b[0;32m    232\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(is_function_like(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m metrics):\n\u001b[1;32m--> 233\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    234\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected all entries in the `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margument_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` list \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    235\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto be metric objects. Received instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    236\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margument_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m             )\n\u001b[0;32m    238\u001b[0m         flat_metrics\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    239\u001b[0m             MetricsList(\n\u001b[0;32m    240\u001b[0m                 [\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    245\u001b[0m             )\n\u001b[0;32m    246\u001b[0m         )\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Expected all entries in the `metrics` list to be metric objects. Received instead:\nmetrics=[[<MeanMetricWrapper name=accuracy>]]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load the pre-trained model\n",
    "model_path = \"Model/model_v1_inceptionV3.h5\"  # Path to the model\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Define dataset paths\n",
    "dataset_path = \"Dataset\"  # Update with your dataset folder\n",
    "train_dir = os.path.join(dataset_path, \"train\")\n",
    "val_dir = os.path.join(dataset_path, \"val\")\n",
    "\n",
    "# Image properties\n",
    "image_size = (299, 299)  # Default size for InceptionV3\n",
    "batch_size = 32  # You can adjust based on your system's capability\n",
    "\n",
    "# Data preprocessing using ImageDataGenerator\n",
    "datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
    "\n",
    "# Prepare train and validation generators\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# Get the class indices (folder names mapped to indices)\n",
    "class_indices = train_generator.class_indices\n",
    "classes = list(class_indices.keys())\n",
    "\n",
    "# Evaluate the model on the validation dataset\n",
    "print(\"Evaluating the model...\")\n",
    "val_loss, val_accuracy = model.evaluate(val_generator, verbose=1)\n",
    "print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Make predictions on the validation set\n",
    "print(\"Making predictions...\")\n",
    "val_generator.reset()  # Reset generator for prediction\n",
    "predictions = model.predict(val_generator, verbose=1)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Get the true labels\n",
    "true_classes = val_generator.classes\n",
    "\n",
    "# Generate a classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_classes, predicted_classes, target_names=classes))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "cm = confusion_matrix(true_classes, predicted_classes)\n",
    "print(cm)\n",
    "\n",
    "# Predict on a single image (example)\n",
    "def predict_image(image_path):\n",
    "    from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "    \n",
    "    # Load and preprocess the image\n",
    "    image = load_img(image_path, target_size=image_size)\n",
    "    image_array = img_to_array(image) / 255.0\n",
    "    image_array = np.expand_dims(image_array, axis=0)\n",
    "\n",
    "    # Predict\n",
    "    prediction = model.predict(image_array)\n",
    "    predicted_class = classes[np.argmax(prediction)]\n",
    "    confidence = np.max(prediction)\n",
    "    \n",
    "    return predicted_class, confidence\n",
    "\n",
    "# Example usage\n",
    "sample_image = \"test/fried_rice/download (1).jpg\"  # Update with a test image path\n",
    "if os.path.exists(sample_image):\n",
    "    predicted_class, confidence = predict_image(sample_image)\n",
    "    print(f\"Predicted Class: {predicted_class}, Confidence: {confidence:.2f}\")\n",
    "else:\n",
    "    print(f\"Sample image '{sample_image}' not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "above not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3996 images belonging to 20 classes.\n",
      "Found 1250 images belonging to 20 classes.\n",
      "Evaluating the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 1s/step - accuracy: 0.9393 - loss: 0.4218\n",
      "Validation Loss: 0.4507, Validation Accuracy: 0.9320\n",
      "Making predictions...\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1s/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       burger       0.98      0.97      0.98        67\n",
      "  butter_naan       0.87      0.89      0.88        61\n",
      "         chai       0.93      0.96      0.94        67\n",
      "      chapati       0.91      0.86      0.88        69\n",
      "chole_bhature       0.98      0.95      0.96        83\n",
      "  dal_makhani       0.91      0.96      0.93        67\n",
      "       dhokla       0.95      0.95      0.95        55\n",
      "   fried_rice       0.95      0.99      0.97        72\n",
      "         idli       0.96      0.97      0.96        67\n",
      "       jalebi       0.98      0.97      0.98        66\n",
      " kaathi_rolls       0.88      0.92      0.90        61\n",
      " kadai_paneer       0.91      0.95      0.93        78\n",
      "        kulfi       0.87      0.84      0.85        49\n",
      "  masala_dosa       0.92      0.93      0.93        60\n",
      "        momos       0.99      0.99      0.99        68\n",
      "   paani_puri       1.00      0.86      0.93        29\n",
      "       pakode       0.93      0.92      0.92        59\n",
      "    pav_bhaji       0.88      0.95      0.91        61\n",
      "        pizza       0.93      0.90      0.92        60\n",
      "       samosa       0.93      0.84      0.89        51\n",
      "\n",
      "     accuracy                           0.93      1250\n",
      "    macro avg       0.93      0.93      0.93      1250\n",
      " weighted avg       0.93      0.93      0.93      1250\n",
      "\n",
      "Confusion Matrix:\n",
      "[[65  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0]\n",
      " [ 0 54  0  5  0  1  0  0  0  0  0  1  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 64  0  0  2  0  0  0  0  0  0  1  0  0  0  0  0  0  0]\n",
      " [ 0  5  1 59  1  1  0  0  0  0  1  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0 79  1  0  1  0  0  0  1  0  1  0  0  0  0  0  0]\n",
      " [ 0  1  0  0  0 64  0  1  0  0  0  0  0  0  0  0  0  1  0  0]\n",
      " [ 0  0  0  0  0  0 52  0  0  0  0  0  1  0  0  0  0  1  1  0]\n",
      " [ 0  0  0  0  0  0  0 71  0  0  0  0  0  0  0  0  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  0 65  0  0  0  2  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 64  0  0  0  0  0  0  1  0  1  0]\n",
      " [ 0  0  0  0  0  0  1  0  0  0 56  0  1  1  0  0  1  0  0  1]\n",
      " [ 0  0  0  1  0  0  1  1  0  0  0 74  0  0  0  0  1  0  0  0]\n",
      " [ 0  0  3  0  0  0  1  0  0  0  1  0 41  2  0  0  0  1  0  0]\n",
      " [ 0  0  0  0  0  1  0  0  0  0  1  0  1 56  0  0  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  1  0  0  0  0  0 67  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  1  0  0  0  0  0  1 25  1  1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  1  0  0  3  0  0  0  0 54  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  1  0  0  0  1  0  1  0  0  0 58  0  0]\n",
      " [ 0  2  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  3 54  0]\n",
      " [ 1  0  0  0  1  0  0  0  0  0  5  1  0  0  0  0  0  0  0 43]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Predicted Class: dal_makhani, Confidence: 0.64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load the pre-trained model\n",
    "model_path = \"Model/model_v1_inceptionV3.h5\"  # Path to the model\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Recompile the model with correct metrics\n",
    "model.compile(\n",
    "    optimizer='adam',  # Or the optimizer you used during training\n",
    "    loss='categorical_crossentropy',  # Loss function used for multi-class classification\n",
    "    metrics=['accuracy']  # Set the metrics explicitly\n",
    ")\n",
    "\n",
    "# Define dataset paths\n",
    "dataset_path = \"Dataset\"  # Update with your dataset folder\n",
    "train_dir = os.path.join(dataset_path, \"train\")\n",
    "val_dir = os.path.join(dataset_path, \"val\")\n",
    "\n",
    "# Image properties\n",
    "image_size = (299, 299)  # Default size for InceptionV3\n",
    "batch_size = 32  # You can adjust based on your system's capability\n",
    "\n",
    "# Data preprocessing using ImageDataGenerator\n",
    "datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
    "\n",
    "# Prepare train and validation generators\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# Get the class indices (folder names mapped to indices)\n",
    "class_indices = train_generator.class_indices\n",
    "classes = list(class_indices.keys())\n",
    "\n",
    "# Evaluate the model on the validation dataset\n",
    "print(\"Evaluating the model...\")\n",
    "val_loss, val_accuracy = model.evaluate(val_generator, verbose=1)\n",
    "print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Make predictions on the validation set\n",
    "print(\"Making predictions...\")\n",
    "val_generator.reset()  # Reset generator for prediction\n",
    "predictions = model.predict(val_generator, verbose=1)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Get the true labels\n",
    "true_classes = val_generator.classes\n",
    "\n",
    "# Generate a classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_classes, predicted_classes, target_names=classes))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "cm = confusion_matrix(true_classes, predicted_classes)\n",
    "print(cm)\n",
    "\n",
    "# Predict on a single image (example)\n",
    "def predict_image(image_path):\n",
    "    from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "    \n",
    "    # Load and preprocess the image\n",
    "    image = load_img(image_path, target_size=image_size)\n",
    "    image_array = img_to_array(image) / 255.0\n",
    "    image_array = np.expand_dims(image_array, axis=0)\n",
    "\n",
    "    # Predict\n",
    "    prediction = model.predict(image_array)\n",
    "    predicted_class = classes[np.argmax(prediction)]\n",
    "    confidence = np.max(prediction)\n",
    "    \n",
    "    return predicted_class, confidence\n",
    "\n",
    "# Example usage\n",
    "sample_image = \"test/fried_rice/download (1).jpg\"  # Update with a test image path\n",
    "if os.path.exists(sample_image):\n",
    "    predicted_class, confidence = predict_image(sample_image)\n",
    "    print(f\"Predicted Class: {predicted_class}, Confidence: {confidence:.2f}\")\n",
    "else:\n",
    "    print(f\"Sample image '{sample_image}' not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is working one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from flask import Flask, request, jsonify, render_template, session\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# Initialize the Flask app\n",
    "app = Flask(__name__)\n",
    "# app.secret_key = 'your_secret_key'  # For session management\n",
    "\n",
    "#new add\n",
    "import pandas as pd\n",
    "allergy_data = pd.read_csv('Food_Allergy_Data.csv')\n",
    "\n",
    "# Load the pre-trained model\n",
    "model_path = \"Model/model_v1_inceptionV3.h5\"  # Update the path to your model\n",
    "food_recognition_model = load_model(model_path)\n",
    "\n",
    "# Define the list of food classes (based on your dataset structure)\n",
    "food_classes = [\n",
    "    \"burger\", \"butter_naan\", \"chai\", \"chapati\", \"chole_bhature\",\n",
    "    \"dal_makhani\", \"dhokla\", \"fried_rice\", \"idli\", \"jalebi\",\n",
    "    \"kaathi_rolls\", \"kadai_paneer\", \"kulfi\", \"masala_dosa\", \"momos\",\n",
    "    \"paani_puri\", \"pakode\", \"pav_bhaji\", \"pizza\", \"samosa\"\n",
    "]\n",
    "\n",
    "\n",
    "# Define routes for your Flask app\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/image_upload')\n",
    "def image_upload():\n",
    "    return render_template('image_upload.html')\n",
    "\n",
    "@app.route('/health')\n",
    "def health():\n",
    "    return render_template('health.html')\n",
    "\n",
    "@app.route('/gender')\n",
    "def gender():\n",
    "    return render_template('gender.html')\n",
    "\n",
    "@app.route('/upload', methods=['POST'])\n",
    "def upload_image():\n",
    "    file = request.files.get('image')  # Get the uploaded file\n",
    "    if file:\n",
    "        try:\n",
    "            # Use the file's stream (convert to BytesIO if necessary)\n",
    "            from io import BytesIO\n",
    "            file_stream = BytesIO(file.read())  # Convert to BytesIO object\n",
    "            \n",
    "            # Load the image using Keras's load_img\n",
    "            image = load_img(file_stream, target_size=(299, 299))  # InceptionV3 input size\n",
    "            image_array = img_to_array(image) / 255.0  # Normalize pixel values to [0, 1]\n",
    "            image_array = np.expand_dims(image_array, axis=0)  # Add batch dimension\n",
    "\n",
    "            # Make predictions using the food recognition model\n",
    "            predictions = food_recognition_model.predict(image_array)\n",
    "            food_index = np.argmax(predictions)\n",
    "\n",
    "            # Get the predicted food label\n",
    "            if food_index < len(food_classes):\n",
    "                food_label = food_classes[food_index]\n",
    "            else:\n",
    "                return jsonify({'error': 'Prediction index out of range'}), 400\n",
    "\n",
    "            # Check for allergens in user health information (stored in session)\n",
    "            user_allergies = session.get('allergies', [])\n",
    "            detected_allergens = detect_allergens(food_label)\n",
    "\n",
    "            # Compare detected allergens with user's allergies\n",
    "            if any(allergen in user_allergies for allergen in detected_allergens):\n",
    "                message = f\"Warning! The food '{food_label}' contains allergens you are allergic to: {', '.join(detected_allergens)}.\"\n",
    "            else:\n",
    "                message = f\"No allergens detected in '{food_label}'. It should be safe to consume.\"\n",
    "\n",
    "            # Render the result page\n",
    "            return render_template('result.html', food_label=food_label, message=message)\n",
    "        except Exception as e:\n",
    "            return jsonify({'error': str(e)}), 500\n",
    "\n",
    "    return jsonify({'error': 'No file uploaded'}), 400\n",
    "\n",
    "#new added\n",
    "def detect_allergens(food_label):\n",
    "    matched_row = allergy_data[allergy_data['Food Item'].str.lower() == food_label.lower()]\n",
    "    \n",
    "    if not matched_row.empty:\n",
    "        allergens = matched_row['Common Allergies'].values[0]\n",
    "        \n",
    "        # Check if allergens is NaN and handle it\n",
    "        if pd.isna(allergens):\n",
    "            return []  # No allergens detected if NaN\n",
    "        \n",
    "        # Split the allergen string into a list\n",
    "        return allergens.split(', ')\n",
    "    \n",
    "    return []  # Return empty list if no match found\n",
    "\n",
    "\n",
    "@app.route('/submit_health_info', methods=['POST'])\n",
    "def submit_health_info():\n",
    "    data = request.json\n",
    "    allergies = data.get('allergies', [])\n",
    "    session['allergies'] = allergies  # Store allergies in the session\n",
    "    return jsonify({'message': 'Health information received'}), 200\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "result,html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Result Page</title>\n",
    "    <link rel=\"stylesheet\" href=\"static/style.css\">\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <!-- Header with navigation tabs -->\n",
    "        <div class=\"header\">\n",
    "            <button class=\"tab\">Food Recognition</button>\n",
    "            <button class=\"tab\">Allergen Detection</button>\n",
    "        </div>\n",
    "\n",
    "        <!-- Result Section -->\n",
    "        <h2>Result</h2>\n",
    "        \n",
    "        <div class=\"result-content\">\n",
    "            <div class=\"result-image\">\n",
    "                <!-- Display the image that was uploaded -->\n",
    "                <img src=\"{{ image_url }}\" alt=\"Food Image\" class=\"food-image\">\n",
    "            </div>\n",
    "\n",
    "            <div class=\"result-details\">\n",
    "                <!-- Display food recognition result -->\n",
    "                <p><strong>Food Recognized:</strong> {{ food_name }}</p>\n",
    "\n",
    "                <!-- Display allergen detection results -->\n",
    "                <p><strong>Allergens Detected:</strong></p>\n",
    "                <ul>\n",
    "                    {% for allergen in allergens %}\n",
    "                        <li>{{ allergen }}</li>\n",
    "                    {% endfor %}\n",
    "                </ul>\n",
    "\n",
    "                <p><strong>Health Recommendations:</strong></p>\n",
    "                <p>{{ health_recommendations }}</p>\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <!-- Back to Home Button -->\n",
    "        <div class=\"button-group\">\n",
    "            <button class=\"upload-btn\" onclick=\"window.location.href='index.html'\">Back to Home</button>\n",
    "        </div>\n",
    "    </div>\n",
    "</body>\n",
    "\n",
    "</html>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
